# Copy this file to .env and fill in your API configuration.
# At least one of the following API key options is required.

# ── Option 1: OpenAI / OpenAI-compatible API (recommended) ───────────────────
# Works with OpenAI, vLLM, Ollama, LM Studio, and any OpenAI-compatible server.

# Your OpenAI API key (or any non-empty string for local vLLM)
# OPENAI_API_KEY=your_openai_api_key_here

# Base URL for the OpenAI-compatible endpoint.
# Leave unset to use the official OpenAI API (https://api.openai.com/v1).
# Examples:
#   vLLM local server : OPENAI_BASE_URL=http://localhost:8000/v1
#   Ollama            : OPENAI_BASE_URL=http://localhost:11434/v1
# OPENAI_BASE_URL=http://localhost:8000/v1

# Model name for LiteLLM. Use openai/<model-name> for OpenAI-compatible endpoints.
# Examples:
#   OpenAI GPT-4o   : AI_MODEL=openai/gpt-4o
#   vLLM local      : AI_MODEL=openai/meta-llama/Llama-3-70b-chat-hf
#   Gemini (LiteLLM): AI_MODEL=gemini/gemini-2.5-flash
# AI_MODEL=openai/gpt-4o

# ── Option 2: Google Gemini API ───────────────────────────────────────────────
# Get your API key at: https://aistudio.google.com/apikey
# GEMINI_API_KEY=your_gemini_api_key_here

# Optional: Use Vertex AI instead of Gemini API
# GOOGLE_GENAI_USE_VERTEXAI=TRUE
