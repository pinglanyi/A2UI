# Copy this file to .env and fill in your API configuration.
# At least one API key is required.

# ── OpenAI / OpenAI-compatible API (recommended) ─────────────────────────────
# Works with OpenAI, vLLM, Ollama, LM Studio, and any OpenAI-compatible server.

# Your API key (required for OpenAI; can be any non-empty string for local vLLM)
OPENAI_API_KEY=your_openai_api_key_here

# Base URL for the OpenAI-compatible endpoint.
# Leave unset to use the official OpenAI API (https://api.openai.com/v1).
# Examples:
#   vLLM local server : OPENAI_BASE_URL=http://localhost:8000/v1
#   Ollama            : OPENAI_BASE_URL=http://localhost:11434/v1
#   LM Studio         : OPENAI_BASE_URL=http://localhost:1234/v1
# OPENAI_BASE_URL=http://localhost:8000/v1

# Model name to use for generation.
# Defaults to "gpt-4o" when OPENAI_API_KEY is set.
# For vLLM specify the model name as served by the endpoint, e.g.:
#   AI_MODEL=meta-llama/Llama-3-70b-chat-hf
# AI_MODEL=gpt-4o

# ── Legacy: Google Gemini API (backward compatibility) ────────────────────────
# If only GEMINI_API_KEY is set (and OPENAI_API_KEY is not), the editor will
# automatically use Google's OpenAI-compatible endpoint with gemini-2.5-flash.
# Get your key at https://aistudio.google.com/apikey
# GEMINI_API_KEY=your_gemini_api_key_here
